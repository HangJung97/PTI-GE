{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net Segmentation - Camus Dataset\n",
    "\n",
    "*Made by **Hang Jung Ling** and **Olivier Bernard** from the INSA Lyon, France.*\n",
    "\n",
    "This notebook shows how to train, test and evaluate a U-Net to segment different cardiac structures on [CAMUS dataset](https://humanheart-project.creatis.insa-lyon.fr/database/#collection/6373703d73e9f0047faa1bc8).\n",
    "\n",
    "CAMUS is one of the largest public echocardiogaphic datasets, with 500 patients and each patient has 4 echocardiographic images: end-diastolic (ED) and end-systolic (ES) frames acquired in both apical two chamber and apical four chamber views. Each image is annotated by an expert and contains 3 classes + background:</br>\n",
    "&emsp;1) Left ventricle</br>\n",
    "&emsp;2) Myocardium</br>\n",
    "&emsp;3) Left atrium</br>\n",
    "\n",
    "Summary :</br>\n",
    "&emsp;I.   [Install dependencies](#install)</br>\n",
    "&emsp;II.  [Dataset](#dataset)</br>\n",
    "&emsp;II.  [Train](#train)</br>\n",
    "&emsp;III. [Visualize](#visualize)</br>\n",
    "&emsp;V.   [Evaluate](#evaluate)</br> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Install dependencies <a class=\"anchor\" id=\"install\"></a>\n",
    "\n",
    "Kindly ignore this step if you have installed your own environment using `environment.yaml`. If not, please execute the following cells to install the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture project_path_setup\n",
    "\n",
    "import sys\n",
    "\n",
    "if \"../\" in sys.path:\n",
    "    print(sys.path)\n",
    "else:\n",
    "    sys.path.append(\"../\")\n",
    "    print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture packages_install\n",
    "\n",
    "# Make sure the repo's package and its dependencies are installed\n",
    "%pip install -e ../."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Dataset <a class=\"anchor\" id=\"dataset\"></a>\n",
    "\n",
    "Once the environment is successfully setup, download the CAMUS dataset by executing the following cell. The dataset will be downloaded to the `data/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Make sure the data is downloaded and extracted where it should be\n",
    "if not Path(\"../data/camus_64\").is_dir():\n",
    "    import zipfile\n",
    "    from io import BytesIO\n",
    "    from urllib.request import urlopen\n",
    "\n",
    "    zipurl = \"https://www.creatis.insa-lyon.fr/~bernard/camus/camus_64.zip\"\n",
    "    with urlopen(zipurl) as zipresp:\n",
    "        with zipfile.ZipFile(BytesIO(zipresp.read())) as zfile:\n",
    "            for member in tqdm(\n",
    "                zfile.infolist(), desc=\"Downloading and extracting data\", position=0, leave=True\n",
    "            ):\n",
    "                try:\n",
    "                    zfile.extract(member, \"../data/\")\n",
    "                except zipfile.error as e:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's split these data into training, validation and testing sets. We will use 80% of the data for training, 10% for validation and 10% for testing. The split is done by patient ID, so that the same patient will not appear in different sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from src.utils.file_and_folder_operations import subdirs\n",
    "\n",
    "# Specify the data directory\n",
    "data_dir = Path(\"../data/camus_64\").resolve()\n",
    "\n",
    "# List all the patients id\n",
    "keys = subdirs(data_dir, prefix=\"patient\", join=False)\n",
    "\n",
    "# Split the patients into 80/10/10 train/val/test sets\n",
    "train_keys, val_and_test_keys = train_test_split(keys, train_size=0.8, random_state=12345)\n",
    "val_keys, test_keys = train_test_split(val_and_test_keys, test_size=0.5, random_state=12345)\n",
    "\n",
    "train_keys = sorted(train_keys)\n",
    "val_keys = sorted(val_keys)\n",
    "test_keys = sorted(test_keys)\n",
    "\n",
    "# Create train, val and test datalist\n",
    "viws_instants = [\"2CH_ED\", \"2CH_ES\", \"4CH_ED\", \"4CH_ES\"]\n",
    "train_datalist = [\n",
    "    {\n",
    "        \"image\": str(data_dir / key / f\"{key}_{view}.nii.gz\"),\n",
    "        \"label\": str(data_dir / key / f\"{key}_{view}_gt.nii.gz\"),\n",
    "    }\n",
    "    for key in train_keys\n",
    "    for view in viws_instants\n",
    "]\n",
    "\n",
    "val_datalist = [\n",
    "    {\n",
    "        \"image\": str(data_dir / key / f\"{key}_{view}.nii.gz\"),\n",
    "        \"label\": str(data_dir / key / f\"{key}_{view}_gt.nii.gz\"),\n",
    "    }\n",
    "    for key in val_keys\n",
    "    for view in viws_instants\n",
    "]\n",
    "\n",
    "test_datalist = [\n",
    "    {\n",
    "        \"image\": str(data_dir / key / f\"{key}_{view}.nii.gz\"),\n",
    "        \"label\": str(data_dir / key / f\"{key}_{view}_gt.nii.gz\"),\n",
    "    }\n",
    "    for key in test_keys\n",
    "    for view in viws_instants\n",
    "]\n",
    "\n",
    "print(\"Example of train keys: \", train_datalist[:2])\n",
    "print(\"Example of validation keys: \", val_datalist[:2])\n",
    "print(\"Example of test keys: \", test_datalist[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is split, we will create a `Dataset` object for each set. This object will be used to load the data during training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from monai.data import CacheDataset\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    EnsureChannelFirstd,\n",
    "    LoadImaged,\n",
    "    NormalizeIntensityd,\n",
    "    RandAdjustContrastd,\n",
    "    RandFlipd,\n",
    "    RandGaussianNoised,\n",
    "    RandGaussianSmoothd,\n",
    "    RandRotated,\n",
    "    RandScaleIntensityd,\n",
    "    RandZoomd,\n",
    ")\n",
    "\n",
    "# Transforms to load data\n",
    "load_transforms = [\n",
    "    LoadImaged(keys=[\"image\", \"label\"], image_only=True),  # Load image and label\n",
    "    EnsureChannelFirstd(\n",
    "        keys=[\"image\", \"label\"]\n",
    "    ),  # Make sure the first dimension is the channel dimension\n",
    "    NormalizeIntensityd(keys=[\"image\"]),  # Normalize the intensity of the image\n",
    "]\n",
    "\n",
    "# Transforms to augment data\n",
    "range_x = [-15.0 / 180 * np.pi, 15.0 / 180 * np.pi]\n",
    "data_augmentation_transforms = [\n",
    "    RandRotated(\n",
    "        keys=[\"image\", \"label\"],\n",
    "        range_x=range_x,\n",
    "        range_y=0,\n",
    "        range_z=0,\n",
    "        mode=[\"bicubic\", \"nearest\"],\n",
    "        padding_mode=\"constant\",\n",
    "        prob=0.2,\n",
    "    ),\n",
    "    RandZoomd(\n",
    "        keys=[\"image\", \"label\"],\n",
    "        min_zoom=0.7,\n",
    "        max_zoom=1.4,\n",
    "        mode=[\"bicubic\", \"nearest\"],\n",
    "        padding_mode=\"constant\",\n",
    "        align_corners=(True, None),\n",
    "        prob=0.2,\n",
    "    ),\n",
    "    RandGaussianNoised(keys=[\"image\"], std=0.01, prob=0.15),\n",
    "    RandGaussianSmoothd(\n",
    "        keys=[\"image\"],\n",
    "        sigma_x=(0.5, 1.15),\n",
    "        sigma_y=(0.5, 1.15),\n",
    "        prob=0.15,\n",
    "    ),\n",
    "    RandScaleIntensityd(keys=[\"image\"], factors=0.3, prob=0.15),\n",
    "    RandAdjustContrastd(keys=[\"image\"], gamma=(0.7, 1.5), prob=0.3),\n",
    "    RandFlipd(keys=[\"image\", \"label\"], spatial_axis=[0], prob=0.5),\n",
    "]\n",
    "\n",
    "# Define transforms for training, validation and testing\n",
    "train_transforms = Compose(load_transforms + data_augmentation_transforms)\n",
    "val_transforms = Compose(load_transforms)\n",
    "test_transforms = Compose(load_transforms)\n",
    "\n",
    "train_ds = CacheDataset(data=train_datalist, transform=train_transforms, cache_rate=1.0)\n",
    "val_ds = CacheDataset(data=val_datalist, transform=val_transforms, cache_rate=1.0)\n",
    "test_ds = CacheDataset(data=test_datalist, transform=test_transforms, cache_rate=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's visualize some images from the training set. The images are displayed with their corresponding ground truth segmentation masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from src.utils.visualizations import imagesc\n",
    "\n",
    "# Get a random image with label from each dataset\n",
    "train_idx = np.random.randint(len(train_ds))\n",
    "val_idx = np.random.randint(len(val_ds))\n",
    "test_idx = np.random.randint(len(test_ds))\n",
    "\n",
    "print(\"train_idx: \", train_idx)\n",
    "print(\"val_idx: \", val_idx)\n",
    "print(\"test_idx: \", test_idx)\n",
    "\n",
    "# Visualize a random image with label from each dataset\n",
    "colors = [\"black\", \"red\", \"green\", \"blue\"]\n",
    "cmap = ListedColormap(colors)\n",
    "\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "train_sample = train_ds[train_idx]\n",
    "image = train_sample[\"image\"].detach().cpu().numpy()[0].transpose(1, 0)\n",
    "label = train_sample[\"label\"].detach().cpu().numpy()[0].transpose(1, 0)\n",
    "ax = figure.add_subplot(3, 2, 1)\n",
    "imagesc(ax, image, title=\"Training image\", show_colorbar=False)\n",
    "ax = figure.add_subplot(3, 2, 2)\n",
    "plt.imshow(label, cmap=cmap, interpolation=\"nearest\")\n",
    "plt.title(\"Training label\")\n",
    "ax.axis(\"off\")\n",
    "\n",
    "val_sample = val_ds[val_idx]\n",
    "image = val_sample[\"image\"].detach().cpu().numpy()[0].transpose(1, 0)\n",
    "label = val_sample[\"label\"].detach().cpu().numpy()[0].transpose(1, 0)\n",
    "ax = figure.add_subplot(3, 2, 3)\n",
    "imagesc(ax, image, title=\"Validation image\", show_colorbar=False)\n",
    "ax = figure.add_subplot(3, 2, 4)\n",
    "plt.imshow(label, cmap=cmap, interpolation=\"nearest\")\n",
    "plt.title(\"Validation label\")\n",
    "ax.axis(\"off\")\n",
    "\n",
    "test_sample = test_ds[test_idx]\n",
    "image = test_sample[\"image\"].detach().cpu().numpy()[0].transpose(1, 0)\n",
    "label = test_sample[\"label\"].detach().cpu().numpy()[0].transpose(1, 0)\n",
    "ax = figure.add_subplot(3, 2, 5)\n",
    "imagesc(ax, image, title=\"Test image\", show_colorbar=False)\n",
    "ax = figure.add_subplot(3, 2, 6)\n",
    "plt.imshow(label, cmap=cmap, interpolation=\"nearest\")\n",
    "plt.title(\"Test label\")\n",
    "ax.axis(\"off\")\n",
    "figure.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Train <a class=\"anchor\" id=\"train\"></a>\n",
    "Let's move on to train a U-Net to segment the left ventricle, myocardium and left atrium. We will use the training and validation sets created in the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of U-Net architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "from src.models.unet import UNet\n",
    "\n",
    "input_channels = 1  # This is the number of input channels in the image\n",
    "input_shape = (input_channels, 64, 64)  # This is the shape of the input image to the network\n",
    "num_classes = 4  # This is the number of output classes\n",
    "output_shape = (num_classes, 64, 64)  # This is the shape of the output mask\n",
    "init_channels = 32  # This is the number of channels in the first layer of the network\n",
    "\n",
    "unet = UNet(input_shape=input_shape, output_shape=output_shape, init_channels=init_channels)\n",
    "\n",
    "# Print the summary of the network\n",
    "summary_kwargs = dict(\n",
    "    col_names=[\"input_size\", \"output_size\", \"kernel_size\", \"num_params\"], depth=3, verbose=0\n",
    ")\n",
    "summary(unet, (1, *input_shape), device=\"cpu\", **summary_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of optimizer, loss function, and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.metrics import DiceMetric\n",
    "\n",
    "# Soft dice and CE loss function\n",
    "loss_function = DiceCELoss(\n",
    "    include_background=False,\n",
    "    to_onehot_y=True,\n",
    "    batch=True,\n",
    "    smooth_nr=0.00001,\n",
    "    smooth_dr=0.00001,\n",
    "    lambda_dice=0.5,\n",
    "    lambda_ce=0.5,\n",
    ")\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = partial(torch.optim.Adam, params=unet.parameters())\n",
    "\n",
    "# Hard dice metric\n",
    "metric = DiceMetric(include_background=False, reduction=\"mean\", get_not_nans=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from typing import Union\n",
    "\n",
    "from monai.data import DataLoader, decollate_batch\n",
    "from monai.transforms import Activations, AsDiscrete\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "def train_process(\n",
    "    train_ds: Dataset,\n",
    "    val_ds: Dataset,\n",
    "    model: nn.Module,\n",
    "    loss_function: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    metric: nn.Module,\n",
    "    num_classes: int,\n",
    "    batch_size: int = 2,\n",
    "    lr: float = 0.001,\n",
    "    max_epochs: int = 30,\n",
    "    log_dir: Union[Path, str] = Path(\"../logs/camus_segmentation\"),\n",
    "    val_interval=1,\n",
    "):\n",
    "    # Create train and validation dataloaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=os.cpu_count() - 1,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=os.cpu_count() - 1,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "    # Determine the device to run the model on\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"GPU detected, training on: {device}!\\n\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"GPU not detected, training on CPU!\\n\")\n",
    "\n",
    "    # Move the model to the device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Finalize the creation of the optimizer object with the learning rate\n",
    "    optimizer = optimizer(lr=lr)\n",
    "\n",
    "    # Define transforms for post processing predictions and labels\n",
    "    post_pred = Compose(\n",
    "        [Activations(softmax=True), AsDiscrete(argmax=True, to_onehot=num_classes)]\n",
    "    )\n",
    "    post_label = Compose([AsDiscrete(to_onehot=num_classes)])\n",
    "\n",
    "    # Convert log directory to Path object if needed\n",
    "    if not isinstance(log_dir, Path):\n",
    "        log_dir = Path(log_dir)\n",
    "    # Create the log directory if it does not exist\n",
    "    log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Define some variables to keep track of the best metric values, epoch time and losses\n",
    "    best_metric = -1\n",
    "    best_metric_epoch = -1\n",
    "    epoch_train_loss_values = []\n",
    "    epoch_val_loss_values = []\n",
    "    metric_values = []\n",
    "    epoch_times = []\n",
    "    epoch_val = []\n",
    "    total_start = time.time()\n",
    "\n",
    "    fit_pbar = tqdm(range(max_epochs), desc=\"Training\", unit=\"epoch\", position=0, leave=True)\n",
    "    pbar_metrics = {\"train/loss\": None, \"val/loss\": None, \"val/metric\": None}\n",
    "    fit_pbar.set_postfix(pbar_metrics)\n",
    "\n",
    "    for epoch in fit_pbar:\n",
    "        epoch_start = time.time()\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        epoch_val_loss = 0\n",
    "        step = 0\n",
    "        for batch_data in train_dataloader:\n",
    "            step += 1\n",
    "            inputs, labels = (\n",
    "                batch_data[\"image\"].to(device),\n",
    "                batch_data[\"label\"].to(device),\n",
    "            )\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            pbar_metrics[\"loss\"] = loss.item()\n",
    "            pbar_metrics[\"train_batch\"] = f\"{step}/{len(train_dataloader)}\"\n",
    "            fit_pbar.set_postfix(pbar_metrics)\n",
    "\n",
    "        epoch_loss /= step\n",
    "        epoch_train_loss_values.append(epoch_loss)\n",
    "        pbar_metrics[\"train/loss\"] = epoch_loss\n",
    "        pbar_metrics.pop(\"loss\")\n",
    "        pbar_metrics.pop(\"train_batch\")\n",
    "        fit_pbar.set_postfix(pbar_metrics)\n",
    "\n",
    "        if (epoch + 1) % val_interval == 0:\n",
    "            step = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for val_data in val_dataloader:\n",
    "                    step += 1\n",
    "                    val_inputs, val_labels = (\n",
    "                        val_data[\"image\"].to(device),\n",
    "                        val_data[\"label\"].to(device),\n",
    "                    )\n",
    "                    val_outputs = model(val_inputs)\n",
    "                    val_loss = loss_function(val_outputs, val_labels)\n",
    "                    epoch_val_loss += val_loss.item()\n",
    "                    val_outputs = [post_pred(i) for i in decollate_batch(val_outputs)]\n",
    "                    val_labels = [post_label(i) for i in decollate_batch(val_labels)]\n",
    "                    metric(y_pred=val_outputs, y=val_labels)\n",
    "                    pbar_metrics[\"loss\"] = val_loss.item()\n",
    "                    pbar_metrics[\"val_batch\"] = f\"{step}/{len(val_dataloader)}\"\n",
    "                    fit_pbar.set_postfix(pbar_metrics)\n",
    "\n",
    "                epoch_val_loss /= step\n",
    "                epoch_val_loss_values.append(epoch_val_loss)\n",
    "                epoch_val.append(epoch + 1)\n",
    "                val_metric = metric.aggregate().item()\n",
    "                metric.reset()\n",
    "                metric_values.append(val_metric)\n",
    "                pbar_metrics.pop(\"loss\")\n",
    "                pbar_metrics.pop(\"val_batch\")\n",
    "                pbar_metrics[\"val/loss\"] = epoch_val_loss\n",
    "                pbar_metrics[\"val/metric\"] = val_metric\n",
    "                fit_pbar.set_postfix(pbar_metrics)\n",
    "\n",
    "                if val_metric > best_metric:\n",
    "                    best_metric = val_metric\n",
    "                    best_metric_epoch = epoch + 1\n",
    "                    torch.save(\n",
    "                        model.state_dict(),\n",
    "                        str(log_dir / \"best_metric_model.pth\"),\n",
    "                    )\n",
    "                    # print(\"saved new best metric model at epoch\", best_metric_epoch)\n",
    "        epoch_times.append(time.time() - epoch_start)\n",
    "\n",
    "    print(\n",
    "        f\"train completed, best_metric: {best_metric:.4f}\"\n",
    "        f\" at epoch: {best_metric_epoch}\"\n",
    "        f\" total time: {(time.time() - total_start):.4f}\"\n",
    "    )\n",
    "    return (\n",
    "        max_epochs,\n",
    "        time.time() - total_start,\n",
    "        epoch_train_loss_values,\n",
    "        epoch_val_loss_values,\n",
    "        epoch_val,\n",
    "        metric_values,\n",
    "        epoch_times,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of training hyperparameters\n",
    "In this section, we will define the hyperparameters for training, such as the number of epochs, the learning rate, the batch size, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "lr = 0.001\n",
    "max_epochs = 30\n",
    "\n",
    "# Train the model\n",
    "(\n",
    "    max_epochs,\n",
    "    total_time,\n",
    "    epoch_train_loss_values,\n",
    "    epoch_val_loss_values,\n",
    "    epoch_val,\n",
    "    metric_values,\n",
    "    epoch_times,\n",
    ") = train_process(\n",
    "    train_ds=train_ds,\n",
    "    val_ds=val_ds,\n",
    "    model=unet,\n",
    "    loss_function=loss_function,\n",
    "    optimizer=optimizer,\n",
    "    metric=metric,\n",
    "    num_classes=num_classes,\n",
    "    batch_size=batch_size,\n",
    "    lr=lr,\n",
    "    max_epochs=max_epochs,\n",
    "    log_dir=\"../logs/camus_segmentation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pti",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
