{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net Segmentation - Camus Dataset\n",
    "\n",
    "*Made by **Hang Jung Ling** and **Olivier Bernard** from the INSA Lyon, France.*\n",
    "\n",
    "This notebook shows how to train, test and evaluate a U-Net to segment different cardiac structures on [CAMUS dataset](https://humanheart-project.creatis.insa-lyon.fr/database/#collection/6373703d73e9f0047faa1bc8).\n",
    "\n",
    "CAMUS is one of the largest public echocardiogaphic datasets, with 500 patients and each patient has 4 echocardiographic images: end-diastolic (ED) and end-systolic (ES) frames acquired in both apical two chamber and apical four chamber views. Each image is annotated by an expert and contains 3 classes + background:</br>\n",
    "&emsp;1) Left ventricle</br>\n",
    "&emsp;2) Myocardium</br>\n",
    "&emsp;3) Left atrium</br>\n",
    "\n",
    "Summary :</br>\n",
    "&emsp;I.   [Install dependencies](#install)</br>\n",
    "&emsp;II.  [Dataset](#dataset)</br>\n",
    "&emsp;II.  [Train](#train)</br>\n",
    "&emsp;III. [Visualize](#visualize)</br>\n",
    "&emsp;V.   [Evaluate](#evaluate)</br> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Install dependencies <a class=\"anchor\" id=\"install\"></a>\n",
    "\n",
    "Kindly ignore this step if you have installed your own environment using `environment.yaml`. If not, please execute the following cells to install the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture project_path_setup\n",
    "\n",
    "import sys\n",
    "\n",
    "if \"../\" in sys.path:\n",
    "    print(sys.path)\n",
    "else:\n",
    "    sys.path.append(\"../\")\n",
    "    print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture packages_install\n",
    "\n",
    "# Make sure the repo's package and its dependencies are installed\n",
    "%pip install -e ../."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Dataset <a class=\"anchor\" id=\"dataset\"></a>\n",
    "\n",
    "Once the environment is successfully setup, download the CAMUS dataset by executing the following cell. The dataset will be downloaded to the `data/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Make sure the data is downloaded and extracted where it should be\n",
    "if not Path(\"../data/camus_64.zip\").is_file():\n",
    "    !wget \"https://www.creatis.insa-lyon.fr/~bernard/camus/camus_64.zip\" --directory-prefix=\"../data/\"\n",
    "    !unzip -qq ../data/camus_64.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's split these data into training, validation and testing sets. We will use 80% of the data for training, 10% for validation and 10% for testing. The split is done by patient ID, so that the same patient will not appear in different sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from src.utils.file_and_folder_operations import subdirs\n",
    "\n",
    "# Specify the data directory\n",
    "data_dir = Path(\"../data/camus_64\").resolve()\n",
    "\n",
    "# List all the patients id\n",
    "keys = subdirs(data_dir, prefix=\"patient\", join=False)\n",
    "\n",
    "# Split the patients into 80/10/10 train/val/test sets\n",
    "train_keys, val_and_test_keys = train_test_split(keys, train_size=0.8, random_state=12345)\n",
    "val_keys, test_keys = train_test_split(val_and_test_keys, test_size=0.5, random_state=12345)\n",
    "\n",
    "train_keys = sorted(train_keys)\n",
    "val_keys = sorted(val_keys)\n",
    "test_keys = sorted(test_keys)\n",
    "\n",
    "# Create train, val and test datalist\n",
    "viws_instants = [\"2CH_ED\", \"2CH_ES\", \"4CH_ED\", \"4CH_ES\"]\n",
    "train_datalist = [\n",
    "    {\n",
    "        \"image\": str(data_dir / key / f\"{key}_{view}.nii.gz\"),\n",
    "        \"label\": str(data_dir / key / f\"{key}_{view}_gt.nii.gz\"),\n",
    "    }\n",
    "    for key in train_keys\n",
    "    for view in viws_instants\n",
    "]\n",
    "\n",
    "val_datalist = [\n",
    "    {\n",
    "        \"image\": str(data_dir / key / f\"{key}_{view}.nii.gz\"),\n",
    "        \"label\": str(data_dir / key / f\"{key}_{view}_gt.nii.gz\"),\n",
    "    }\n",
    "    for key in val_keys\n",
    "    for view in viws_instants\n",
    "]\n",
    "\n",
    "test_datalist = [\n",
    "    {\n",
    "        \"image\": str(data_dir / key / f\"{key}_{view}.nii.gz\"),\n",
    "        \"label\": str(data_dir / key / f\"{key}_{view}_gt.nii.gz\"),\n",
    "    }\n",
    "    for key in test_keys\n",
    "    for view in viws_instants\n",
    "]\n",
    "\n",
    "print(\"Example of train keys: \", train_datalist[:5])\n",
    "print(\"Example of validation keys: \", val_datalist[:5])\n",
    "print(\"Example of test keys: \", test_datalist[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is split, we will create a `Dataset` object for each set. This object will be used to load the data during training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from monai.data import CacheDataset\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    EnsureChannelFirstd,\n",
    "    LoadImaged,\n",
    "    NormalizeIntensityd,\n",
    "    RandAdjustContrastd,\n",
    "    RandFlipd,\n",
    "    RandGaussianNoised,\n",
    "    RandGaussianSmoothd,\n",
    "    RandRotated,\n",
    "    RandScaleIntensityd,\n",
    "    RandZoomd,\n",
    ")\n",
    "\n",
    "# Transforms to load data\n",
    "load_transforms = [\n",
    "    LoadImaged(keys=[\"image\", \"label\"], image_only=True),  # Load image and label\n",
    "    EnsureChannelFirstd(\n",
    "        keys=[\"image\", \"label\"]\n",
    "    ),  # Make sure the first dimension is the channel dimension\n",
    "    NormalizeIntensityd(keys=[\"image\"]),  # Normalize the intensity of the image\n",
    "]\n",
    "\n",
    "# Transforms to augment data\n",
    "range_x = [-15.0 / 180 * np.pi, 15.0 / 180 * np.pi]\n",
    "data_augmentation_transforms = [\n",
    "    RandRotated(\n",
    "        keys=[\"image\", \"label\"],\n",
    "        range_x=range_x,\n",
    "        range_y=0,\n",
    "        range_z=0,\n",
    "        mode=[\"bicubic\", \"nearest\"],\n",
    "        padding_mode=\"constant\",\n",
    "        prob=0.2,\n",
    "    ),\n",
    "    RandZoomd(\n",
    "        keys=[\"image\", \"label\"],\n",
    "        min_zoom=0.7,\n",
    "        max_zoom=1.4,\n",
    "        mode=[\"bicubic\", \"nearest\"],\n",
    "        padding_mode=\"constant\",\n",
    "        align_corners=(True, None),\n",
    "        prob=0.2,\n",
    "    ),\n",
    "    RandGaussianNoised(keys=[\"image\"], std=0.01, prob=0.15),\n",
    "    RandGaussianSmoothd(\n",
    "        keys=[\"image\"],\n",
    "        sigma_x=(0.5, 1.15),\n",
    "        sigma_y=(0.5, 1.15),\n",
    "        prob=0.15,\n",
    "    ),\n",
    "    RandScaleIntensityd(keys=[\"image\"], factors=0.3, prob=0.15),\n",
    "    RandAdjustContrastd(keys=[\"image\"], gamma=(0.7, 1.5), prob=0.3),\n",
    "    RandFlipd(keys=[\"image\", \"label\"], spatial_axis=[0], prob=0.5),\n",
    "]\n",
    "\n",
    "# Define transforms for training, validation and testing\n",
    "train_transforms = Compose(load_transforms + data_augmentation_transforms)\n",
    "val_transforms = Compose(load_transforms)\n",
    "test_transforms = Compose(load_transforms)\n",
    "\n",
    "train_ds = CacheDataset(data=train_datalist, transform=train_transforms, cache_rate=1.0)\n",
    "val_ds = CacheDataset(data=val_datalist, transform=val_transforms, cache_rate=1.0)\n",
    "test_ds = CacheDataset(data=test_datalist, transform=test_transforms, cache_rate=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's visualize some images from the training set. The images are displayed with their corresponding ground truth segmentation masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from src.utils.visualizations import imagesc\n",
    "\n",
    "# Get a random image with label from each dataset\n",
    "train_idx = np.random.randint(len(train_ds))\n",
    "val_idx = np.random.randint(len(val_ds))\n",
    "test_idx = np.random.randint(len(test_ds))\n",
    "\n",
    "print(\"train_idx: \", train_idx)\n",
    "print(\"val_idx: \", val_idx)\n",
    "print(\"test_idx: \", test_idx)\n",
    "\n",
    "# Visualize a random image with label from each dataset\n",
    "colors = [\"black\", \"red\", \"green\", \"blue\"]\n",
    "cmap = ListedColormap(colors)\n",
    "\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "train_sample = train_ds[train_idx]\n",
    "image = train_sample[\"image\"].detach().cpu().numpy()[0].transpose(1, 0)\n",
    "label = train_sample[\"label\"].detach().cpu().numpy()[0].transpose(1, 0)\n",
    "ax = figure.add_subplot(3, 2, 1)\n",
    "imagesc(ax, image, title=\"Training image\", show_colorbar=False)\n",
    "ax = figure.add_subplot(3, 2, 2)\n",
    "plt.imshow(label, cmap=cmap, interpolation=\"nearest\")\n",
    "plt.title(\"Training label\")\n",
    "ax.axis(\"off\")\n",
    "\n",
    "val_sample = val_ds[val_idx]\n",
    "image = val_sample[\"image\"].detach().cpu().numpy()[0].transpose(1, 0)\n",
    "label = val_sample[\"label\"].detach().cpu().numpy()[0].transpose(1, 0)\n",
    "ax = figure.add_subplot(3, 2, 3)\n",
    "imagesc(ax, image, title=\"Validation image\", show_colorbar=False)\n",
    "ax = figure.add_subplot(3, 2, 4)\n",
    "plt.imshow(label, cmap=cmap, interpolation=\"nearest\")\n",
    "plt.title(\"Validation label\")\n",
    "ax.axis(\"off\")\n",
    "\n",
    "test_sample = test_ds[test_idx]\n",
    "image = test_sample[\"image\"].detach().cpu().numpy()[0].transpose(1, 0)\n",
    "label = test_sample[\"label\"].detach().cpu().numpy()[0].transpose(1, 0)\n",
    "ax = figure.add_subplot(3, 2, 5)\n",
    "imagesc(ax, image, title=\"Test image\", show_colorbar=False)\n",
    "ax = figure.add_subplot(3, 2, 6)\n",
    "plt.imshow(label, cmap=cmap, interpolation=\"nearest\")\n",
    "plt.title(\"Test label\")\n",
    "ax.axis(\"off\")\n",
    "figure.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Train <a class=\"anchor\" id=\"train\"></a>\n",
    "Let's move on to train a U-Net to segment the left ventricle, myocardium and left atrium. We will use the training and validation sets created in the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of U-Net architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "from src.models.unet import UNet\n",
    "\n",
    "input_channels = 1  # This is the number of input channels in the image\n",
    "input_shape = (input_channels, 64, 64)  # This is the shape of the input image to the network\n",
    "output_channels = 4  # This is the number of output classes\n",
    "output_shape = (output_channels, 64, 64)  # This is the shape of the output mask\n",
    "init_channels = 32  # This is the number of channels in the first layer of the network\n",
    "\n",
    "unet = UNet(input_shape=input_shape, output_shape=output_shape, init_channels=init_channels)\n",
    "\n",
    "# Print the summary of the network\n",
    "summary_kwargs = dict(\n",
    "    col_names=[\"input_size\", \"output_size\", \"kernel_size\", \"num_params\"], depth=3, verbose=0\n",
    ")\n",
    "summary(unet, (1, *input_shape), device=\"cpu\", **summary_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of optimizer, loss function, and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.metrics import DiceMetric\n",
    "\n",
    "# Soft dice and CE loss function\n",
    "loss_function = DiceCELoss(\n",
    "    include_background=False,\n",
    "    batch=True,\n",
    "    smooth_nr=0.00001,\n",
    "    smooth_dr=0.00001,\n",
    "    lambda_dice=0.5,\n",
    "    lambda_ce=0.5,\n",
    ")\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = torch.optim.Adam(unet.parameters(), lr=0.001)\n",
    "\n",
    "# Hard dice metric\n",
    "metric = DiceMetric(include_background=False, reduction=\"mean\", get_not_nans=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from monai.data import DataLoader, decollate_batch\n",
    "from monai.transforms import AsDiscrete\n",
    "\n",
    "\n",
    "def train_process(\n",
    "    train_ds,\n",
    "    val_ds,\n",
    "    model,\n",
    "    device,\n",
    "    loss_function,\n",
    "    optimizer,\n",
    "    metric,\n",
    "    max_epochs,\n",
    "    log_dir,\n",
    "    val_interval=1,\n",
    "):\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        num_workers=8,\n",
    "    )\n",
    "    val_loader = DataLoader(val_ds, batch_size=1, num_workers=4)\n",
    "    model = model.to(device)\n",
    "    loss_function = DiceCELoss(\n",
    "        include_background=False,\n",
    "        batch=True,\n",
    "        smooth_nr=0.00001,\n",
    "        smooth_dr=0.00001,\n",
    "        lambda_dice=0.5,\n",
    "        lambda_ce=0.5,\n",
    "    )\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=0.1,\n",
    "        momentum=0.9,\n",
    "        weight_decay=0.00004,\n",
    "    )\n",
    "\n",
    "    post_pred = Compose([AsDiscrete(argmax=True, to_onehot=model.output_shape[0])])\n",
    "    post_label = Compose([AsDiscrete(to_onehot=model.output_shape[0])])\n",
    "\n",
    "    dice_metric = DiceMetric(include_background=True, reduction=\"mean\", get_not_nans=False)\n",
    "\n",
    "    best_metric = -1\n",
    "    best_metric_epoch = -1\n",
    "    epoch_loss_values = []\n",
    "    metric_values = []\n",
    "    epoch_times = []\n",
    "    total_start = time.time()\n",
    "    for epoch in range(max_epochs):\n",
    "        epoch_start = time.time()\n",
    "        print(\"-\" * 10)\n",
    "        print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        step = 0\n",
    "        for batch_data in train_loader:\n",
    "            step_start = time.time()\n",
    "            step += 1\n",
    "            inputs, labels = (\n",
    "                batch_data[\"image\"].to(device),\n",
    "                batch_data[\"label\"].to(device),\n",
    "            )\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            print(\n",
    "                f\"{step}/{len(train_ds) // train_loader.batch_size},\"\n",
    "                f\" train_loss: {loss.item():.4f}\"\n",
    "                f\" step time: {(time.time() - step_start):.4f}\"\n",
    "            )\n",
    "        epoch_loss /= step\n",
    "        epoch_loss_values.append(epoch_loss)\n",
    "        print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        if (epoch + 1) % val_interval == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for val_data in val_loader:\n",
    "                    val_inputs, val_labels = (\n",
    "                        val_data[\"image\"].to(device),\n",
    "                        val_data[\"label\"].to(device),\n",
    "                    )\n",
    "                    val_outputs = model(val_inputs)\n",
    "                    val_outputs = [post_pred(i) for i in decollate_batch(val_outputs)]\n",
    "                    val_labels = [post_label(i) for i in decollate_batch(val_labels)]\n",
    "                    dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "                metric = dice_metric.aggregate().item()\n",
    "                dice_metric.reset()\n",
    "                metric_values.append(metric)\n",
    "                if metric > best_metric:\n",
    "                    best_metric = metric\n",
    "                    best_metric_epoch = epoch + 1\n",
    "                    torch.save(\n",
    "                        model.state_dict(),\n",
    "                        str(log_dir / \"best_metric_model.pth\"),\n",
    "                    )\n",
    "                    print(\"saved new best metric model\")\n",
    "                print(\n",
    "                    f\"current epoch: {epoch + 1} current\"\n",
    "                    f\" mean dice: {metric:.4f}\"\n",
    "                    f\" best mean dice: {best_metric:.4f}\"\n",
    "                    f\" at epoch: {best_metric_epoch}\"\n",
    "                )\n",
    "        print(f\"time of epoch {epoch + 1} is: {(time.time() - epoch_start):.4f}\")\n",
    "        epoch_times.append(time.time() - epoch_start)\n",
    "\n",
    "    print(\n",
    "        f\"train completed, best_metric: {best_metric:.4f}\"\n",
    "        f\" at epoch: {best_metric_epoch}\"\n",
    "        f\" total time: {(time.time() - total_start):.4f}\"\n",
    "    )\n",
    "    return (\n",
    "        max_epochs,\n",
    "        time.time() - total_start,\n",
    "        epoch_loss_values,\n",
    "        metric_values,\n",
    "        epoch_times,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pti",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
